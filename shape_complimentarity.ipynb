{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import regex\n",
    "import glob\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import copy\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sys import stderr\n",
    "from numpy import linalg\n",
    "import networkx as nx\n",
    "import re\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull\n",
    "#key functions\n",
    "from utils.Hypersphere import fit_hypersphere\n",
    "from utils.read_msms import read_msms\n",
    "from Bio.PDB.ResidueDepth import ResidueDepth\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import Delaunay, ConvexHull\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "import unittest\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Font of the Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 13\n",
    "BIGGER_SIZE = 13\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(x):\n",
    "    return x/np.linalg.norm(x)\n",
    "\n",
    "def unit_normal(v1, v2, v3):\n",
    "    v1, v2, v3 = np.array(v1), np.array(v2), np.array(v3)\n",
    "    v31 = v3 - v1\n",
    "    v21 = v2 - v1\n",
    "    cross = np.cross(v31/np.linalg.norm(v31), v21/np.linalg.norm(v21))\n",
    "    return cross/ np.linalg.norm(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Input and the Output Folder Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it exists\n"
     ]
    }
   ],
   "source": [
    "input_folder_name = 'Protein_protein'\n",
    "output_folder_name = 'output_protein_protein'\n",
    "\n",
    "path = Path(os.path.abspath('./data/'))\n",
    "sub_path = Path(os.path.abspath('./data/'+input_folder_name))\n",
    "OUTPUT = output_folder_name\n",
    "try:\n",
    "    os.mkdir(os.path.expanduser(path/OUTPUT))\n",
    "    # this is making the directory of that name\n",
    "except FileExistsError:\n",
    "    print(\"it exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DataFrame of the list of IDs and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1cdt</td>\n",
       "      <td>/home/prathith/Documents/protein_curvature_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1cdt_lig</td>\n",
       "      <td>/home/prathith/Documents/protein_curvature_pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_id                                          file_path\n",
       "0      1cdt  /home/prathith/Documents/protein_curvature_pro...\n",
       "1  1cdt_lig  /home/prathith/Documents/protein_curvature_pro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_of_dms_files = pd.DataFrame(columns=['file_id','file_path'])\n",
    "for files in glob.glob(sub_path.as_posix()+\"/*.dms\"): #gets files having .dms extension in the said directory\n",
    "    filename = files\n",
    "    if filename in list(df_of_dms_files['file_path']):\n",
    "        break\n",
    "    else:\n",
    "        structure_id = regex.search(\n",
    "            r\"(?:.+[/\\\\])(.+)(?:\\.dms)\", filename).group(1) \n",
    "        s1 = structure_id #s1 becomes structure ID\n",
    "        new_df = pd.DataFrame({'file_id':[s1],'file_path':[filename]})\n",
    "        df_of_dms_files = pd.concat([df_of_dms_files,new_df],ignore_index=True)\n",
    "#     df_of_dms_files=pd.concat(df_of_dms_files,pd.DataFrame({'file_id':s1,'file_path':filename}),ignore_index=True)\n",
    "df_of_dms_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the _ X .pdb file in the Output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdb_X_file(filename,s1):\n",
    "    import re\n",
    "    import numpy as np\n",
    "    with open(filename, 'r') as f:\n",
    "        k = f.read()\n",
    "    # pattern is created by the compile function in re\n",
    "    pattern = re.compile(r\"(.{20,})(?:\\bA\\b)\", flags=re.M | re.I)\n",
    "    pattern2 = re.compile(r\"(.{20,})(?:\\bS\\w+\\b)(.+)\", flags=re.M)\n",
    "    \n",
    "    l1 = pattern.findall(k) #k which is the filename, we are finding whether the characters that are defined by the pattern are found\n",
    "    l2 = pattern2.findall(k)\n",
    "    iterables1 = {}\n",
    "    iterables_orig = {}\n",
    "    iterables_normal_area = {}\n",
    "\n",
    "    for x, y in l2:\n",
    "        search = regex.search(\n",
    "            r\"(\\w{,3})\\s*(\\w+)(?:\\*?)\\s*(\\w+)(?:\\*|'?)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\", x)\n",
    "        iterables1[tuple(map(float, [search.group(4), search.group(5), search.group(6)]))] = [\n",
    "            x, list(map(float, y.split()[:]))]\n",
    "        \n",
    "    \n",
    "\n",
    "    for x in l1:\n",
    "        search = regex.search(\n",
    "            r\"(\\w{,3})\\s*(\\w+)(?:\\*?)\\s*(\\w+)(?:\\*|'?)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\", x)\n",
    "        iterables_orig[tuple(\n",
    "            map(float, [search.group(4), search.group(5), search.group(6)]))] = [x]\n",
    "    pattern_new = regex.compile(\n",
    "        r\"(\\w{3})\\s*(\\w+)\\s*(\\w+)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\")\n",
    "    data = np.array([x for x in iterables1.keys()])\n",
    "    data = np.array(data, 'float64')\n",
    "    Z = linkage(data, 'complete')  # ward --> complete\n",
    "    max_d = 5  # patch\n",
    "    clusters = fcluster(Z, max_d, criterion='distance')\n",
    "    curvature = collections.defaultdict(list)\n",
    "    centroid = np.median(data, axis=0)\n",
    "    with open(os.path.expanduser(path.as_posix()+'/%s/%s_%s.pdb' % (OUTPUT, s1, 'X')), 'w') as f:\n",
    "        dist = []\n",
    "        j = 0\n",
    "        for i in range(1, max(clusters)+1):\n",
    "            curv1_p = []\n",
    "            curv2_p = []\n",
    "            curv_m = fit_hypersphere(data[clusters == i])\n",
    "            ci = curv_m[1]\n",
    "            count = []\n",
    "            d_centroid = np.linalg.norm(centroid-ci)\n",
    "\n",
    "            for x in data[clusters == i]:\n",
    "\n",
    "                d = np.linalg.norm(ci-x)\n",
    "                d_c = np.linalg.norm(centroid-x)\n",
    "                if d_c > d_centroid:\n",
    "                    # if d>curv_m[0]:\n",
    "                    count.append(1)\n",
    "                    \n",
    "                    curv1_p.append(x)\n",
    "                else:\n",
    "                    count.append(-1)\n",
    "                    curv2_p.append(x)\n",
    "\n",
    "            A = (len(curv1_p)/len(data[clusters == i]))\n",
    "            B = (len(curv2_p)/len(data[clusters == i]))\n",
    "            for x in curv1_p:\n",
    "                curvature[tuple(x)] = A*100/curv_m[0]**1\n",
    "            for x in curv2_p:\n",
    "                curvature[tuple(x)] = B*-100/curv_m[0]**1  # put - sign\n",
    "\n",
    "    j = 0\n",
    "    with open(os.path.expanduser(path.as_posix()+'/%s/%s_%s.pdb' % (OUTPUT, s1, 'X')), 'w') as f:\n",
    "        for _, x in enumerate(curvature.keys()):\n",
    "\n",
    "            loc1 = iterables1[tuple(x)]\n",
    "            loc = loc1[0].split()\n",
    "            print(\"{:6s}{:5d} {:^4s}{:1s}{:3s} {:1s}{:4d}{:1s}   {:8.3f}{:8.3f}{:8.3f}{:6.2f}{:6.2f}          {:>2s}{:2s}\".format(\"ATOM\", j, \"A\", \" \", loc[0], \"X\",\n",
    "                                                                                                                                  int(loc[1].rstrip(regex.search(r'(\\d+)(.*)', loc[1]).group(2))), '', x[0], x[1], x[2], loc1[1][0],  curvature[tuple(x)], '', loc[2]), file=f)\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    dist = [curvature[x] for x in curvature]\n",
    "    dist = np.array(dist)\n",
    "    dots = len(dist)\n",
    "    plt.figure()\n",
    "    plt.xlabel(\n",
    "        \"Curvature($\\kappa$)\\n$\\longleftarrow$ concave | convex $\\longrightarrow$\")\n",
    "    plt.ylabel(\"number of surface points\")\n",
    "    plt.title('%s %s:Number of surface points: %d\\nScaling factor: 100*$\\kappa$' %\n",
    "              (s1.upper(), \"\", len(dist)))\n",
    "    plt.hist(dist, bins=15, color='gray', alpha=0.8)\n",
    "    plt.savefig(os.path.expanduser(path.as_posix()+'/%s/%s_%s_%s hist.jpeg' %\n",
    "                                   (OUTPUT, dots, s1, 'X')), format='jpeg', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the final Histogram of Shape Complimentarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Protein - Ligand\n",
    "\n",
    "\"\"\"\n",
    "def generate_the_complimentarity_plot():\n",
    "    \n",
    "    pdb_id = collections.defaultdict(list)\n",
    "    dms_id = collections.defaultdict(list)\n",
    "    dms_path = os.path.expanduser(sub_path.as_posix()+\"/*.dms\")\n",
    "\n",
    "    for files in glob.glob(dms_path):\n",
    "        filename = files\n",
    "        structure_id = regex.search(\n",
    "            r\"(?:.+[/\\\\])(.+)(?:\\.dms)\", filename, flags=regex.I).group(1)\n",
    "        s1 = structure_id\n",
    "        dms_id[structure_id].append(filename)\n",
    "\n",
    "    dms_normal = {}\n",
    "    for name_dms in dms_id:\n",
    "        with open(dms_id[name_dms][0], 'r') as f:\n",
    "            k = f.read()\n",
    "        pattern = re.compile(r\"(.{20,})(?:\\bA\\b)\", flags=re.M | re.I)\n",
    "        pattern2 = re.compile(r\"(.{20,})(?:\\bS\\w+\\b)(.+)\", flags=re.M)\n",
    "        l1 = pattern.findall(k)\n",
    "        l2 = pattern2.findall(k)\n",
    "        iterables1 = {}\n",
    "        iterables_orig = {}\n",
    "        iterables_normal_area = {}\n",
    "        for x, y in l2:\n",
    "            search = regex.search(\n",
    "                r\"(\\w{,3})\\s*(\\w+)(?:\\*?)\\s*(\\w+)(?:\\*|'?)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\\s*(-?\\d+\\.\\d+)\", x)\n",
    "            iterables1[tuple(map(float, [search.group(4), search.group(\n",
    "                5), search.group(6)]))] = list(map(float, y.split()[1:]))\n",
    "        dms_normal[name_dms] = copy.deepcopy(iterables1)\n",
    "\n",
    "    for files in glob.glob((path/OUTPUT).as_posix()+\"/*.pdb\"):\n",
    "        filename = files\n",
    "        structure_id = regex.search(\n",
    "            r\"(?:.+/)(.{4})(?:.*_X\\.pdb)\", filename).group(1)\n",
    "        pdb_id[structure_id].append(filename)\n",
    "\n",
    "\n",
    "    for name_pdb in pdb_id:\n",
    "        for i in itertools.combinations(pdb_id[name_pdb], 2):\n",
    "            with open(i[0], 'r') as f:\n",
    "                k = f.readlines()\n",
    "            iterables = {}\n",
    "            arr1 = []\n",
    "            arr1_norm = []\n",
    "            if(regex.search(r\"_lig_X\", i[0])):\n",
    "                suffix = \"_lig\"\n",
    "            else:\n",
    "                suffix = \"\"\n",
    "            for x in k:\n",
    "                iterables.setdefault(x[60:66].replace(\" \", \"\"), []).append(list\n",
    "                                                                           (map(float, [x[30:38].replace(\" \", \"\"),\n",
    "                                                                                        x[38:46].replace(\n",
    "                                                                                            \" \", \"\"),\n",
    "                                                                                        x[46:54].replace(\" \", \"\")])))\n",
    "                arr1.append(list(map(float, [x[30:38].replace(\" \", \"\"),\n",
    "                                             x[38:46].replace(\" \", \"\"),\n",
    "                                             x[46:54].replace(\" \", \"\"), x[60:66].replace(\" \", \"\")])))\n",
    "                arr1_norm.append(dms_normal[name_pdb+suffix][tuple(map(float, [x[30:38].replace(\" \", \"\"),\n",
    "                                                                               x[38:46].replace(\n",
    "                                                                                   \" \", \"\"),\n",
    "                                                                               x[46:54].replace(\" \", \"\")]))])\n",
    "            with open(i[1], 'r') as f:\n",
    "                k1 = f.readlines()\n",
    "\n",
    "            iterables1 = {}\n",
    "            arr2 = []\n",
    "            arr2_norm = []\n",
    "            if(regex.search(r\"_lig_X\", i[1])):\n",
    "                suffix = \"_lig\"\n",
    "            else:\n",
    "                suffix = \"\"\n",
    "            for x in k1:\n",
    "                iterables1.setdefault(x[60:66].replace(\" \", \"\"), []).append(list\n",
    "                                                                            (map(float, [x[30:38].replace(\" \", \"\"),\n",
    "                                                                                         x[38:46].replace(\n",
    "                                                                                             \" \", \"\"),\n",
    "                                                                                         x[46:54].replace(\" \", \"\")])))\n",
    "                arr2.append(list(map(float, [x[30:38].replace(\" \", \"\"),\n",
    "                                             x[38:46].replace(\" \", \"\"),\n",
    "                                             x[46:54].replace(\" \", \"\"), x[60:66].replace(\" \", \"\")])))\n",
    "                arr2_norm.append(dms_normal[name_pdb+suffix][tuple(map(float, [x[30:38].replace(\" \", \"\"),\n",
    "                                                                               x[38:46].replace(\n",
    "                                                                                   \" \", \"\"),\n",
    "                                                                               x[46:54].replace(\" \", \"\")]))])\n",
    "            arr1 = np.array(arr1)\n",
    "            arr2 = np.array(arr2)\n",
    "            arr1_norm = np.array(arr1_norm)\n",
    "            arr2_norm = np.array(arr2_norm)\n",
    "            print('works')\n",
    "            normal_product = np.dot(arr2_norm, arr1_norm.T)\n",
    "\n",
    "#             arr_dist = distance.cdist(\n",
    "#                 arr2[:, (0, 1, 2)], arr1[:, (0, 1, 2)], 'euclidean')\n",
    "\n",
    "#             new_dist = np.exp(-1*(arr_dist-np.mean(arr_dist, axis=0))\n",
    "#                               ** 2/(2*np.var(arr_dist, axis=0)))\n",
    "\n",
    "#             new_curv = distance.cdist(arr2[:, (3,)], -1*arr1[:, (3,)], 'cityblock')\n",
    "#             dat_new = (np.multiply(new_curv, new_dist)).flatten()\n",
    "#             plt.figure(dpi=300)\n",
    "\n",
    "#             plt.xlabel(\"shape complementarity\")\n",
    "#             plt.ylabel(\"Number density\")\n",
    "#             plt.hist(dat_new, bins=20, density=True, color='gray', alpha=0.8)\n",
    "#             plt.title(\"%s_%s\" % (name_pdb, \"_lig\"))\n",
    "\n",
    "#             plt.savefig(path.as_posix()+\"/%s_%s_plot.jpeg\" %\n",
    "#                         (name_pdb, \"_lig\"), format='jpeg', \n",
    "#                         dpi=300)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1cdt_lig'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = df_of_dms_files.iloc[1][1]\n",
    "s1 = df_of_dms_files.iloc[1][0]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write_pdb_X_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrite_pdb_X_file\u001b[49m(filename,s1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write_pdb_X_file' is not defined"
     ]
    }
   ],
   "source": [
    "write_pdb_X_file(filename,s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.9 GiB for an array with shape (54556, 56380) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_the_complimentarity_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mgenerate_the_complimentarity_plot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m arr2_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(arr2_norm)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m normal_product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr2_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr1_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 22.9 GiB for an array with shape (54556, 56380) and data type float64"
     ]
    }
   ],
   "source": [
    "generate_the_complimentarity_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr1_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43marr1_norm\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr1_norm' is not defined"
     ]
    }
   ],
   "source": [
    "arr1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
